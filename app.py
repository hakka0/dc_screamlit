import streamlit as st
import pandas as pd
import boto3
import io
import datetime
from botocore.config import Config

st.set_page_config(page_title="ê°¤ëŸ¬ë¦¬ ëŒ€ì‹œë³´ë“œ", layout="wide")

st_header_col, st_space, st_date_col, st_time_col = st.columns([5, 1, 2, 3])

with st_header_col:
    st.title("ðŸ“Š ê°¤ëŸ¬ë¦¬ í™œë™ ëŒ€ì‹œë³´ë“œ")

@st.cache_data(ttl=300)
def load_data_from_r2():
    try:
        aws_access_key_id = st.secrets["CF_ACCESS_KEY_ID"]
        aws_secret_access_key = st.secrets["CF_SECRET_ACCESS_KEY"]
        account_id = st.secrets["CF_ACCOUNT_ID"]
        bucket_name = st.secrets["CF_BUCKET_NAME"]
    except KeyError:
        st.error("Secrets ì„¤ì • ì˜¤ë¥˜")
        return pd.DataFrame()

    s3 = boto3.client(
        's3',
        endpoint_url=f'https://{account_id}.r2.cloudflarestorage.com',
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        config=Config(signature_version='s3v4')
    )

    try:
        response = s3.list_objects_v2(Bucket=bucket_name)
    except Exception as e:
        st.error(f"R2 ì ‘ì† ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

    all_dfs = []
    
    if 'Contents' in response:
        files = [f for f in response['Contents'] if f['Key'].endswith('.xlsx')]
        if not files:
            return pd.DataFrame()
            
        for file in files:
            file_key = file['Key']
            try:
                obj = s3.get_object(Bucket=bucket_name, Key=file_key)
                data = obj['Body'].read()
                df = pd.read_excel(io.BytesIO(data))
                all_dfs.append(df)
            except:
                continue
    
    if not all_dfs:
        return pd.DataFrame()

    final_df = pd.concat(all_dfs, ignore_index=True)
    final_df['ìˆ˜ì§‘ì‹œê°„'] = pd.to_datetime(final_df['ìˆ˜ì§‘ì‹œê°„'])
    return final_df

df = load_data_from_r2()

if not df.empty:
    min_date = df['ìˆ˜ì§‘ì‹œê°„'].dt.date.min()
    max_date = df['ìˆ˜ì§‘ì‹œê°„'].dt.date.max()

    with st_date_col:
        selected_date = st.date_input(
            "ðŸ“… ë‚ ì§œ ì„ íƒ",
            value=max_date, min_value=min_date, max_value=max_date
        )

    with st_time_col:
        start_hour, end_hour = st.slider(
            "â° ì‹œê°„ëŒ€ ì„ íƒ",
            0, 24, (0, 24), step=1, format="%dì‹œ"
        )

    # ë°ì´í„° í•„í„°ë§
    day_filtered_df = df[df['ìˆ˜ì§‘ì‹œê°„'].dt.date == selected_date]
    
    if end_hour == 24:
        filtered_df = day_filtered_df[day_filtered_df['ìˆ˜ì§‘ì‹œê°„'].dt.hour >= start_hour]
    else:
        filtered_df = day_filtered_df[
            (day_filtered_df['ìˆ˜ì§‘ì‹œê°„'].dt.hour >= start_hour) & 
            (day_filtered_df['ìˆ˜ì§‘ì‹œê°„'].dt.hour < end_hour)
        ]

    st.markdown("---")

    if filtered_df.empty:
        st.warning(f"âš ï¸ {selected_date} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # [ìˆ˜ì •ë¨] KPI ê³„ì‚° (ì¤‘ë³µ ì œê±°ëœ ìˆœìˆ˜ ìœ ì € ìˆ˜)
        total_posts = filtered_df['ìž‘ì„±ê¸€ìˆ˜'].sum()
        total_comments = filtered_df['ìž‘ì„±ëŒ“ê¸€ìˆ˜'].sum()
        
        # ì—¬ê¸°ì„œ nunique()ë¥¼ ì“°ë©´ í•´ë‹¹ ê¸°ê°„ ë‚´ ì¤‘ë³µ í™œë™ìžëŠ” 1ëª…ìœ¼ë¡œ ê³„ì‚°ë¨
        active_users = filtered_df['ID(IP)'].nunique()

        col1, col2, col3 = st.columns(3)
        col1.metric("ðŸ“ ì´ ê²Œì‹œê¸€", f"{total_posts:,}ê°œ")
        col2.metric("ðŸ’¬ ì´ ëŒ“ê¸€", f"{total_comments:,}ê°œ")
        col3.metric("ðŸ‘¥ ìˆœìˆ˜ í™œë™ ìœ ì €", f"{active_users:,}ëª…") # ë¼ë²¨ ë³€ê²½

        tab1, tab2, tab3 = st.tabs(["ðŸ“ˆ ì‹œê°„ëŒ€ë³„ ì¶”ì´", "ðŸ† ìœ ì € ëž­í‚¹", "ðŸ° ìœ ì € íƒ€ìž… ë¹„ìœ¨"])

        with tab1:
            st.subheader(f"{selected_date} ì‹œê°„ëŒ€ë³„ í™œë™ ì§€í‘œ")
            
            # [í•µì‹¬ ìˆ˜ì •] ì‹œê°„ëŒ€ë³„ ì§‘ê³„ ë°©ì‹ ë³€ê²½
            # ID(IP) ì»¬ëŸ¼ì— nunique í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°ëœ ìœ ì € ìˆ˜ë¥¼ êµ¬í•¨
            time_agg = filtered_df.groupby('ìˆ˜ì§‘ì‹œê°„').agg({
                'ìž‘ì„±ê¸€ìˆ˜': 'sum',
                'ìž‘ì„±ëŒ“ê¸€ìˆ˜': 'sum',
                'ID(IP)': 'nunique'
            }).rename(columns={'ID(IP)': 'í™œë™ìœ ì €ìˆ˜'})
            
            # ê·¸ëž˜í”„ ê·¸ë¦¬ê¸°
            st.line_chart(time_agg)
            
            # (ì˜µì…˜) ë°ì´í„°í”„ë ˆìž„ìœ¼ë¡œë„ ë³´ì—¬ì£¼ê¸° (í™•ì¸ìš©)
            with st.expander("ìƒì„¸ ë°ì´í„° ë³´ê¸°"):
                st.dataframe(time_agg)

        with tab2:
            st.subheader("ðŸ”¥ í™œë™ì™• ëž­í‚¹ (Top 20)")
            # ëž­í‚¹ì€ ë‹¨ìˆœížˆ í•©ì‚°í•˜ë©´ ë˜ë¯€ë¡œ ê¸°ì¡´ ìœ ì§€ (ë§Žì´ í™œë™í•œ ì‚¬ëžŒì´ë‹ˆê¹Œ ì¤‘ë³µ í•©ì‚°ì´ ë§žìŒ)
            user_df = filtered_df.groupby(['ë‹‰ë„¤ìž„', 'ID(IP)', 'ìœ ì €íƒ€ìž…'])[['ì´í™œë™ìˆ˜', 'ìž‘ì„±ê¸€ìˆ˜', 'ìž‘ì„±ëŒ“ê¸€ìˆ˜']].sum().reset_index()
            top_users = user_df.sort_values(by='ì´í™œë™ìˆ˜', ascending=False).head(20)
            
            st.dataframe(
                top_users,
                column_config={
                    "ì´í™œë™ìˆ˜": st.column_config.ProgressColumn(format="%d", min_value=0, max_value=int(top_users['ì´í™œë™ìˆ˜'].max()) if not top_users.empty else 100),
                },
                hide_index=True, use_container_width=True
            )

        with tab3:
            st.subheader("ðŸ“Š ê³ ë‹‰ vs ìœ ë™ ë¹„ìœ¨ (ìˆœìˆ˜ ìœ ì € ê¸°ì¤€)")
            # ìœ ì € íƒ€ìž… ë¹„ìœ¨ë„ 'í™œë™ íšŸìˆ˜' ê¸°ì¤€ì´ ì•„ë‹ˆë¼ 'ì‚¬ëžŒ ë¨¸ë¦¿ìˆ˜' ê¸°ì¤€ìœ¼ë¡œ ë³´ê³  ì‹¶ë‹¤ë©´ ì•„ëž˜ì²˜ëŸ¼ ìˆ˜ì •
            # ì¤‘ë³µ ì œê±° í›„ ìœ ì € íƒ€ìž… ì„¸ê¸°
            unique_users = filtered_df.drop_duplicates(subset=['ID(IP)'])
            type_counts = unique_users['ìœ ì €íƒ€ìž…'].value_counts()
            
            # ë§Œì•½ í™œë™ëŸ‰(ê¸€+ëŒ“ê¸€) ê¸°ì¤€ ë¹„ìœ¨ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ì•„ëž˜ ì£¼ì„ í•´ì œ í›„ ìœ„ 2ì¤„ ì£¼ì„ ì²˜ë¦¬
            # type_counts = filtered_df['ìœ ì €íƒ€ìž…'].value_counts()
            
            st.bar_chart(type_counts)

else:
    st.info("ë°ì´í„° ë¡œë”© ì¤‘...")
